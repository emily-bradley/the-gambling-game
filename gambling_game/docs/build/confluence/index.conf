<p>The Gambling Game is a hands-on learning exercise for the Data Science Community of Practice. It is designed to foster collaborative exploration of the data science topic: Reinforcement Learning. To participate in the event, follow the steps under <ac:link ac:anchor="HowSubmitanAgent">
<ac:link-body>How Submit an Agent</ac:link-body>
</ac:link>.</p><p>If you want to learn more about the Gambling Game, read on. If you are interested in getting started, check out the <ac:link>
<ri:page ri:content-title="Getting Started" />
<ac:link-body>getting started</ac:link-body>
</ac:link> section for further information. The game framework details are available under <ac:link>
<ri:page ri:content-title="Usage" />
<ac:link-body>usage</ac:link-body>
</ac:link>.</p><h1>The Gambling Game</h1>
<p>You are a player in a casino. However, this casino is no ordinary casino. It is composed entirely of slot machines. Furthermore, these slot machines have a pre-determined and static probability of dispensing a reward. And they all pay out at the same rate - $1000! It occurs to you that, if you can find the machines with the highest probability of giving a reward, you can receive more rewards over time. This is your chance to finally beat the house! There is only one problem - you have limited resources. These slot machines only accept special tokens, of which you only have 1000. You start to wonder, given your limited resources, how you might develop a strategy for maximizing the amount of money you leave the casino with…</p><h1>Background</h1>
<p>The casino scenario is an example of a classic data science problem: the multi-armed bandit (MAB) problem. In MAB, there exists a set of bandits (such as slot machines) which have some hidden probability of dispensing a reward. During each round, the player can choose one bandit, or arm, to “pull”. The selected choice will either dispense a reward (1000 in our case) or return no reward (0). The objective is to develop an algorithm that maximizes the rewards earned over the entire game. Successful strategies can quickly identify the machine or set of machines with the highest likelihood of reward.</p><p>This problem demonstrates the data science concept of the exploration/exploitation tradeoff. Some portion of time (i.e. rounds or tokens) must be spent “exploring” bandits in order to further our understanding of the reward probability. A strategy that <em>always</em> selects a bandit at random would be an example of an “explore only” strategy. This strategy is demonstrated in the simulation’s example agent (ExampleAgent):</p><ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[self.static_selection = random.choice(self.bandits)

def action(self):
    return self.static_selection]]></ac:plain-text-body>
</ac:structured-macro>
<p>Likewise, a strategy that <em>always</em> selects the same bandit would be an example of an “exploit only” strategy. This strategy is demonstrated in another example agent (ExampleAgent2):</p><ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[def action(self):
    bandit_choice = random.choice(self.bandits)
    return bandit_choice]]></ac:plain-text-body>
</ac:structured-macro>
<p>MAB is a reinforcement learning problem, which means a player will receive feedback after each choice is made. This paradigm enables your agent to update its algorithm each round using feedback from the environment. Updating your agent can enable more sophisticated approaches than the two defined above.</p><p>In this game, we represent an over-simplified version of the problem, in which the reward probabilities are non-changing and rewards remain the same across all machines. However, this simulation lends itself to expansions on the problem, which are applicable to a variety of real-world problems such as:</p><ul>
<li>
<p>design of experimental trials</p></li>
<li>
<p>media recommendations (i.e. at movies or music)</p></li>
<li>
<p>advertisement personalization</p></li>
</ul>
<h2>Data Science Terms and Concepts</h2>
<ul>
<li>
<p>reinforcement learning</p></li>
<li>
<p>explore/exploit</p></li>
<li>
<p>regret</p></li>
<li>
<p>limited resource allocation dilemma</p></li>
<li>
<p>online learning</p></li>
<li>
<p>convergence</p></li>
<li>
<p>long-tail effect</p></li>
</ul>
<h1>Game Framework</h1>
<p>The <code>run_game.py</code> is the main file in this program; it runs imports and executes the various classes. run_game takes 2 parameters: <code>bandits</code> and <code>rounds</code>. <code>bandits</code> represents the number of bandits to initialize. This will be the number of available slot machines to explore. The default is 3 slot machines. <code>rounds</code> is an integer that represents the number of “chances” or “tokens” your agent will be given. Default is 1000 rounds.</p><p>Execute a simulation on the command line:</p><ac:structured-macro ac:name="code">
<ac:parameter ac:name="language">python</ac:parameter>
<ac:parameter ac:name="linenumbers">false</ac:parameter>
<ac:plain-text-body><![CDATA[python3 run_game.py --bandits 3 --rounds 1000]]></ac:plain-text-body>
</ac:structured-macro>
<p>The <code>Gamble</code> class is the heart of the game; it controls each round with <code>execute_round</code>. <code>Gamble</code> represents the environment of the reinforcement learning problem. It retains the <code>Agent</code> and <code>Bandit</code> objects, and tracks the performance of each competitor over time.</p><p>The <code>Bandit</code> class lays out the structure for a slot machine bandit. Bandits are named in sequential order (i.e. 1, 2, 3, etc.).</p><p>The <code>Agent</code> class sets the structure for competing in the game. <strong>The agent.py file is the *only* file you need to edit</strong> You will be responsible for developing your own implementation of an agent class. The Agent ABC class must be inherited; all agents should implement an <code>action</code> and <code>update</code> method. The <code>action</code> method returns a bandit to pull. The <code>update</code> method takes the reward information that results from an action.</p><p><strong>note</strong>: A surprise change will be introduced half-way through the competition! It would behoove you to think generally about the problem.</p><h1>How to Win</h1>
<p>The game objective is to maximize the amount of rewards over time, or to <strong>minimize regret</strong>. Regret is the difference between the rewards an agent received and the total rewards possible, given the optimal choice was selected each round.</p><p>$regret = sum_{i=0}^{n} max(A) - sum_{i=0}^{n} hat{a}$</p><p>Regret is the difference between the sum of all rewards possible and the sum of rewards received, where A is the set of all possible actions and $hat{a}$ is the action selected or “pulled” at each time step.</p><h2>How Submit an Agent</h2>
<h3>1. Create or join a team!</h3>
<ul>
<li>
<p>Fill out <a href="https://forms.office.com/Pages/ResponsePage.aspx?id=3Vb9ntvpv0OER2DUQTtZSpwZgkLsSVpNmp0ZHYTk3ntUOUkxUVRBS1RIQzk4RzhQRUNPMlhBTjJKUy4u">this form</a> to join the event</p></li>
</ul>
<h3>2. Check out a feature branch</h3>
<ul>
<li>
<p>Clone the <a href="https://dev.azure.com/Standard-Insurance/DataScience/_git/data-science-cop?path=/games/gambling_game">data_science_cop repo</a> locally:  <code>git clone https://Standard-Insurance@dev.azure.com/Standard-Insurance/DataScience/_git/data-science-cop</code></p></li>
<li>
<p>Checkout a feature branch: <code>git checkout -b my_team_name</code></p></li>
</ul>
<h3>3. Install Requirements</h3>
<ul>
<li>
<p>Create a virtual environment <code>python -m venv .venv</code></p></li>
<li>
<p>Activate the virtual environment <code>source .venv/bin/activate</code></p></li>
<li>
<p>Install requirements: <code>pip install -r requirements.txt</code></p></li>
<li>
<p>Test the game: <code>python run_game.py</code></p></li>
</ul>
<h3>4. Build and Agent</h3>
<ul>
<li>
<p>Work with your team to create a new agent instance <strong>inside ``agent.py``</strong></p><ol style="list-style-type: decimal;">
<li>
<p><strong>Copy the template</strong> from <code>agent_template.py</code> and paste it into <code>agent.py</code></p></li>
<li>
<p><strong>Name your agent</strong> class and the <code>name</code> attribute, which will be displayed on the leaderboard.</p></li>
<li>
<p><strong>Develop an action strategy</strong> that will choose a bandit object when called</p></li>
<li>
<p><strong>Develop an update strategy</strong> that will update your algorithm, give a bandit choice and the reward</p></li>
</ol>
</li>
</ul>
<h3>4. Test your competitor</h3>
<ul>
<li>
<p>Execute <code>python run_game.py</code> to import your competitor(s) from <code>agent.py</code></p></li>
<li>
<p><strong>Tip</strong>: You can develop multiple competitors and have them compete against eachother</p></li>
<li>
<p><code>run_game.py</code> can take in 3 parameters: <code>bandits</code>, <code>rounds</code>, and <code>plot</code></p><ul>
<li>
<p><code>-b</code> number of bandits to initialize</p></li>
<li>
<p><code>-r</code> number of rounds to run</p></li>
<li>
<p><code>-p</code> plot indicator (boolean value)</p></li>
<li>
<p>example: <code>python run_game.py -r 100000 -b 100 -p True</code></p></li>
</ul>
</li>
<li>
<p>Run <code>pytest test_classes.py</code> before submitting a PR</p></li>
</ul>
<h3>5. Submit a pull request</h3>
<ul>
<li>
<p>To submit your agent, create a PR off of your feature branch into main before the due date</p></li>
<li>
<p>Final day to submit: <strong>August 4th, 2023</strong></p></li>
<li>
<p>Results Announced: <strong>August 7th, 2023</strong></p></li>
<li>
<p>Algorithms Presentation: <strong>August 9th, 2023</strong></p></li>
</ul>
<p><strong>note:</strong> If your agent wins, please be prepared to give a brief description of the algorithm during the August meeting.</p><h1>Further Research and Resources</h1>
<ul>
<li>
<p><a href="https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/">Article: What is the Multi-Armed Bandit Problem</a></p></li>
<li>
<p><a href="https://www.youtube.com/watch?v=bkw6hWvh_3k">Video: Multi-Armed Bandit Intro</a></p></li>
<li>
<p><a href="https://www.youtube.com/watch?v=VrFZCGCwzVc">Video: A/B Testing vs Multi-Armed Bandit</a></p></li>
<li>
<p><a href="https://www.youtube.com/watch?v=KoMKgNeUX4k">Real-World Use Case: Spotify Multi-Objective Contextual Bandits Use Case</a></p></li>
<li>
<p><a href="https://multithreaded.stitchfix.com/blog/2020/08/05/bandits/">Real-World Use Case: Stitch Fix Multi-Armed Bandit Recommendations</a></p></li>
<li>
<p><a href="https://www.jmlr.org/papers/volume5/mannor04b/mannor04b.pdf">Research Paper: The Sample Complexity of Exploration in the Multi-Armed Bandit Problem</a></p></li>
</ul>
